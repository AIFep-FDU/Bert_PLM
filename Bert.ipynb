{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtDIHI1XUxbQ"
   },
   "source": [
    "# Pretrained Language Model (PLM)\n",
    "Recently, there are mainly two types of learning paradigm in NLP: pretrain-then-finetuning and pretrain-prompt-predict. Both of them requires the costly pretraining phase, and the difference is if we need additional task-specific training (i.e., finetuning) or not (i.e., prompt-predict).\n",
    "\n",
    "Since pretraining is very costly, there are many open-source PLMs available, such as [Huggingface](https://huggingface.co/docs/transformers/index). Today, we will learn how to use these PLMs for downstream tasks. Note that each PLM has two separate learned models:\n",
    "\n",
    "*   Pre-trained tokenizer\n",
    "*   Pre-trained model\n",
    "\n",
    "These two models must be paired well! Otherwise, the input vocabulary will be different and cause errors.\n",
    "\n",
    "Here are illustrations of the two paradigms:\n",
    "\n",
    "## pretrain-then-finetuning\n",
    "![](https://drive.google.com/uc?export=view&id=1DdxxKb15LUofLw3fgKcPw7xrx8ClczHy)\n",
    "\n",
    "Pretrain-then-finetuning will re-use the parameters (except the output layer) for downstream tasks. For a specific task, e.g., text classification, here are the steps:\n",
    "\n",
    "1.   We instantiate a tokenizer, whose parameters are loaded from a pre-trained tokenizer.\n",
    "2.   we randomly initialize a network that has the same architecture with the PLM. Optionally, the output layer may be different or the same.\n",
    "3.   Load the pre-trained parameters from PLM.\n",
    "    * if the final output layer is the same, we also copy the parameters.\n",
    "    * if the final output layer is different, we randomly initialize.\n",
    "4.   Finetuning. The loaded parameters can be freeze (*required_grad=False*) or tuned (*required_grad=True*). Note that if the final output layer is different, we must tune them.\n",
    "\n",
    "## pretrain-prompt-predict\n",
    "![](https://drive.google.com/uc?export=view&id=1vaa3cPB30X7esBpkl392YtuBTNat8qHk)\n",
    "\n",
    "Pretrain-prompt-predict will re-use all the parameters including the output layer for downstream tasks. And, there will be no further training. The only thing you can do is to design different input template or the verbalizer for better performance.\n",
    "\n",
    "## prompt-based tuning\n",
    "Alternatively, there are a new trend to finetuning a small amount of parameters following pretrain-prompt-predict paradigm. The tuned parameters are called soft prompt (e.g., some special tokens in the inputs or randomly initialized hidden vectors in layers). Note that this is different from finetuning because these is no additional output layer.\n",
    "\n",
    "Today, we will learn how to use Huggingface pre-trained BERT model for a text classification task: Natural Language Inference (NLI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCBjENHZTw97",
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1684902094588,
     "user_tz": -480,
     "elapsed": 6995,
     "user": {
      "displayName": "Dr CAO Yixin _",
      "userId": "02213333189428679281"
     }
    },
    "outputId": "a426bc62-5087-4c49-d0bf-3f5120458284"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 134434559501533850\n",
       " xla_global_id: -1,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 14343274496\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 14250291311337272897\n",
       " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
       " xla_global_id: 416903419]"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "#@title show your CPU or GPU details\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23099,
     "status": "ok",
     "timestamp": 1684902121170,
     "user": {
      "displayName": "Dr CAO Yixin _",
      "userId": "02213333189428679281"
     },
     "user_tz": -480
    },
    "id": "sBPkanKCrWlu",
    "outputId": "ccf611ed-fced-4eb8-9a3e-b6833789a919",
    "cellView": "form"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/SMU_MITB_NLP/week9\n"
     ]
    }
   ],
   "source": [
    "#@title connect google drive folder\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/SMU_MITB_NLP/week9/"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BERT for text classification\n",
    "\n",
    "[BERT](https://arxiv.org/abs/1810.04805) is a deep Bidirectional Transformers that can encode a sequence of words and output their contextualized representations. It opens an era of pretraining-then-finetuning and achieves great success in many downstream tasks.\n",
    "\n",
    "Hugging Face Transformers is a Python library that provides many PLMs including BERT. We can use them for text classification, token classification, masked language, question answer, or even obtain the output hidden states for custom BERT. [Here](https://huggingface.co/docs/transformers/v4.29.1/en/model_doc/bert#transformers.BertConfig) are various implementations based on BERT.\n",
    "\n",
    "Except for this tutorial, here is another detailed [reference colab tutorial](https://colab.research.google.com/drive/1pxc-ehTtnVM72-NViET_D2ZqOlpOi2LH?usp=sharing#scrollTo=5WzqhpquoD4E).\n",
    "\n",
    "It is also recommended to request a **GPU** for training.\n",
    "\n",
    "\n",
    "We're going to go through a few use cases:\n",
    "* [Tokenizers](https://huggingface.co/docs/transformers/main_classes/tokenizer)\n",
    "* BERT Models\n",
    "* Finetuning."
   ],
   "metadata": {
    "id": "baUxXfUQ63Zw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# install hugging face packages\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install accelerate"
   ],
   "metadata": {
    "id": "qyjjDUXNcukz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1684902165596,
     "user_tz": -480,
     "elapsed": 34059,
     "user": {
      "displayName": "Dr CAO Yixin _",
      "userId": "02213333189428679281"
     }
    },
    "outputId": "335a6152-eb97-4a1e-e2c5-313e16989968"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.1/7.1 MB\u001B[0m \u001B[31m100.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m224.5/224.5 kB\u001B[0m \u001B[31m26.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.8/7.8 MB\u001B[0m \u001B[31m103.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m474.6/474.6 kB\u001B[0m \u001B[31m34.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
      "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m110.5/110.5 kB\u001B[0m \u001B[31m18.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m212.5/212.5 kB\u001B[0m \u001B[31m28.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m134.3/134.3 kB\u001B[0m \u001B[31m22.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m16.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.14.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
      "Collecting responses<0.19 (from datasets)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m114.5/114.5 kB\u001B[0m \u001B[31m18.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m268.8/268.8 kB\u001B[0m \u001B[31m29.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m149.6/149.6 kB\u001B[0m \u001B[31m22.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m219.1/219.1 kB\u001B[0m \u001B[31m13.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.19.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title import packages\n",
    "import transformers\n",
    "from transformers import BertTokenizer, AutoTokenizer, BertModel, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from datasets import load_metric\n",
    "from accelerate import Accelerator\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "from typing import List, Optional, Union\n",
    "import dataclasses\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "transformers.logging.set_verbosity_error()\n"
   ],
   "metadata": {
    "cellView": "form",
    "id": "HwrXl0nvceho"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizer\n",
    "The tokenizers take raw strings (e.g., sentences) and output a list of tokens in the vocabulary as the model inputs.\n",
    "\n",
    "You can access tokenizers either with model-specific Tokenizer class (e.g., *BertTokenizer* from BERT model), or with the AutoTokenizer class to decide the tokenizer class automatically."
   ],
   "metadata": {
    "id": "qsPHXE_vdWD2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#@title tokenizer instantiation\n",
    "emp_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') # convenient! Defaults to Fast\n",
    "print(emp_tokenizer)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183,
     "referenced_widgets": [
      "e4d621fb5c4a4f1f95b8dd9d1880bfc6",
      "91faa8e3badb43409bb8317ebf9e660e",
      "f94088ac96ca4b3284c0afa4e1b1d5d1",
      "a3e5154ab9e4436a877bfcfeccabe1b9",
      "610005c89d9447e182ca6b17f357b536",
      "f95e593057904a7a90dec4a74481936f",
      "afbe7115f0094c918eb1c62dbc46cec1",
      "bdbbbe8b7e9148e397b6ab8ab4583c7e",
      "6756e4c228404afcac500e1ec73bbd28",
      "1bf7564c134d4102b6e36396d3e6010a",
      "aba8686e374344fd92c54be6ffcf6b06",
      "768af08be5f14513adb3630e022da6f2",
      "b5b4c15d22064d3389e60667b97f2ebe",
      "829671f4f2064c7a9124ea3fa8b2bdb5",
      "ce36752e5aa54b9eaff08dfca8a86daa",
      "2f97926f132946c0b57c3bda1c779d94",
      "2ad341e0d068424383537d9064cff21f",
      "347e38295a4f46deaf2dad05c2c90ac8",
      "949d9179d15a45a584d0753181f3c939",
      "b75ffda82d7a44eca6116331fc044c00",
      "507fd8aa9d1b429886ab0e3bf42ffa02",
      "d5430bb1f217419b820e26cd029b4291",
      "51ccb77a3715446cb3a56a33e7f714ec",
      "5f6e21b3ad4d44e0ab3e141c51020979",
      "8b88cbf5d89e467c8bb8384b3dae1bbd",
      "23a22917e8d14145961544f31805a3e4",
      "a7c8149c62164561b059fdd75aec7cfe",
      "ed33ebf045ef4cba9d701d1b095fdc73",
      "2603f9a0d3d54d9d821f4de029810660",
      "ba4d2a9337a54accbdc742714532a48c",
      "8f865859b89d40ca8c36431d71cc56f2",
      "8ad61a856afe4bb58ac3491d405b0628",
      "695bc58e0d74493b82c9ffb4e7335bc8",
      "c80925436eb94b5895cc436bb54ceb6e",
      "b02ad3c5a27f4936a92d33f2cd2063d4",
      "dd21df22626c4aa9aff11f0eb8797648",
      "4546c2b074e04f88b0332fbf3fd6feba",
      "b570c8c2ad7544b99cc481ebfccf837a",
      "75188feeee7045b59e40d0c57940e400",
      "f5c93d3d3322400094e4e4aa6f4ffcdd",
      "576d73b34397431881558516e7dc2168",
      "c75d1f618f774367bfab5b04b6634d35",
      "ae67b233395d409b9dfd6b94e4c8a76a",
      "ec02a2a4f0a84336b9bd67cf6e93ab5c"
     ]
    },
    "id": "3XmEKasvehhe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1684902185413,
     "user_tz": -480,
     "elapsed": 4583,
     "user": {
      "displayName": "Dr CAO Yixin _",
      "userId": "02213333189428679281"
     }
    },
    "outputId": "888d8f4b-27bd-44ea-f3a4-262ac25181b8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e4d621fb5c4a4f1f95b8dd9d1880bfc6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "768af08be5f14513adb3630e022da6f2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51ccb77a3715446cb3a56a33e7f714ec"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c80925436eb94b5895cc436bb54ceb6e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title tokenize a sentence\n",
    "emp_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text = \"HuggingFace Transformers is great!\"\n",
    "emp_tokens = emp_tokenizer.tokenize(text)\n",
    "emp_ids = emp_tokenizer.convert_tokens_to_ids(emp_tokens)\n",
    "emp_ids_special_tokens = [emp_tokenizer.cls_token_id] + emp_ids + [emp_tokenizer.sep_token_id]\n",
    "decoded_str = emp_tokenizer.decode(emp_ids_special_tokens)\n",
    "\n",
    "\n",
    "print(\"tokenize:             \", emp_tokens)\n",
    "print(\"convert_tokens_to_ids:\", emp_ids)\n",
    "print(\"add special tokens:   \", emp_ids_special_tokens)\n",
    "print(\"--------\")\n",
    "print(\"decode:               \", decoded_str)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5B-rm7-EdRyX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1684902190239,
     "user_tz": -480,
     "elapsed": 718,
     "user": {
      "displayName": "Dr CAO Yixin _",
      "userId": "02213333189428679281"
     }
    },
    "outputId": "d6d925a6-f83e-4f37-a055-f4264923423b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tokenize:              ['hugging', '##face', 'transformers', 'is', 'great', '!']\n",
      "convert_tokens_to_ids: [17662, 12172, 19081, 2003, 2307, 999]\n",
      "add special tokens:    [101, 17662, 12172, 19081, 2003, 2307, 999, 102]\n",
      "--------\n",
      "decode:                [CLS] huggingface transformers is great! [SEP]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title tokenize for dataset curation\n",
    "\n",
    "emp_premise = \"No Weapons of Mass Destruction Found in Iraq Yet.\"\n",
    "emp_hypothesis = \"Weapons of Mass Destruction Found in Iraq.\"\n",
    "emp_p_tokens = emp_tokenizer(emp_premise, return_tensors=\"pt\")\n",
    "emp_h_tokens = emp_tokenizer(emp_hypothesis, return_tensors=\"pt\")\n",
    "emp_tokens = emp_tokenizer.encode_plus(\n",
    "                emp_premise,\n",
    "                emp_hypothesis,\n",
    "                truncation='longest_first',\n",
    "                add_special_tokens=True,\n",
    "                max_length=20,\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "\n",
    "print(\"premise token ids:    \", emp_p_tokens)\n",
    "print(\"hypothesis token ids: \", emp_h_tokens)\n",
    "print(\"truncated token ids:  \", emp_tokens)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lpPhQQ_yeyN-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1684902457695,
     "user_tz": -480,
     "elapsed": 848,
     "user": {
      "displayName": "Dr CAO Yixin _",
      "userId": "02213333189428679281"
     }
    },
    "outputId": "904195b5-56b0-4ec9-b24e-a4151fb179de"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "premise token ids:     {'input_ids': tensor([[ 101, 2053, 4255, 1997, 3742, 6215, 2179, 1999, 5712, 2664, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "hypothesis token ids:  {'input_ids': tensor([[ 101, 4255, 1997, 3742, 6215, 2179, 1999, 5712, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "truncated token ids:   {'input_ids': tensor([[ 101, 2053, 4255, 1997, 3742, 6215, 2179, 1999, 5712, 2664,  102, 4255,\n",
      "         1997, 3742, 6215, 2179, 1999, 5712, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BERT Models\n",
    "\n",
    "Initializing models is very similar to initializing tokenizers. You can either use the model class specific to your model (e.g., `BertModel`) or you can use an AutoModel class.\n",
    "\n",
    "For different downstream tasks, Hugging Face sets up the model classes with different heads. The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension, usually composed of one or a few linear layers.\n",
    "\n",
    "* ForMaskedLM\n",
    "* ForMultipleChoice\n",
    "* ForQuestionAnswering\n",
    "* ForSequenceClassification\n",
    "* ForTokenClassification\n",
    "* ... [more](https://huggingface.co/docs/transformers/model_doc/auto)\n",
    "\n",
    "> For example, `BertForSequenceClassification` will takes whole sequence embeddings as inputs, and outputs a predefined number of logits for classification. [codes](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py)\n",
    "\n",
    "Note that different models require different input formats. Read the documents carefully!\n",
    "\n"
   ],
   "metadata": {
    "id": "g1SLO_jEdmAV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#@title PLM instantiation\n",
    "\n",
    "emp_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "tmp_outputs = emp_model(**emp_tokens)\n",
    "\n",
    "print('inputs: ', emp_tokenizer.decode(emp_tokens['input_ids'][0]))\n",
    "print('outputs: ', tmp_outputs)\n",
    "print(f\"output distribution over labels: {torch.softmax(tmp_outputs.logits, dim=1)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wd0nTL-odd4N",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1684902521006,
     "user_tz": -480,
     "elapsed": 5037,
     "user": {
      "displayName": "Dr CAO Yixin _",
      "userId": "02213333189428679281"
     }
    },
    "outputId": "320145af-f539-4243-8804-c40f99194d0b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "inputs:  [CLS] no weapons of mass destruction found in iraq yet [SEP] weapons of mass destruction found in iraq. [SEP]\n",
      "outputs:  SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9704, -0.3823]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "output distribution over labels: tensor([[0.7946, 0.2054]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title model architecture\n",
    "\n",
    "print(emp_model)"
   ],
   "metadata": {
    "id": "Mxh-itA4yG-w",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1684902769066,
     "user_tz": -480,
     "elapsed": 526,
     "user": {
      "displayName": "Dr CAO Yixin _",
      "userId": "02213333189428679281"
     }
    },
    "outputId": "59fbe06e-40cd-4df6-e736-4a4c15708989"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finetune for NLI\n",
    "\n",
    "Basically, there are four components:\n",
    "\n",
    "1.   prepare data\n",
    "2.   prepare model\n",
    "3.   finetuning\n",
    "4.   evaluate\n",
    "\n",
    "More examples can be found in [link](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue_no_trainer.py)\n",
    "\n"
   ],
   "metadata": {
    "id": "PQZjDSEnKgMc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Natural Language Inference\n",
    "NLI, also called textual entailment, is a kind of text classification task. Given two sentences (a premise and a hypothesis), NLI is to predict if we can infer the hypothesis from the premise (a.k.a. entailment), or not (Some datasets define the \"not\" class as *non-entailment*, like RTE, while others define the \"not\" class with two classes: *contradiction* and *neutral*).\n",
    "\n",
    "Here are two examples from RTE:\n",
    "\n",
    "*   Premise: No Weapons of Mass Destruction Found in Iraq Yet.\n",
    "*   Hypothesis: Weapons of Mass Destruction Found in Iraq.\n",
    "*   Label: not_entailment\n",
    "---\n",
    "*   Premise: Lin Piao, after all, was the creator of Mao's \"Little Red Book\" of quotations.\n",
    "*   Hypothesis: Lin Piao wrote the \"Little Red Book\".\n",
    "*   Label: entailment\n",
    "---\n",
    "\n",
    "*   Dataset: [Recognizing Textual Entailment (RTE)](https://gluebenchmark.com/tasks), place the three downloaded files (train.tsv, dev.tsv, test.tsv) into the folder \"/content/drive/MyDrive/SMU_MITB_NLP/week9/RTE/\".\n"
   ],
   "metadata": {
    "id": "QGa_ybrFZUo5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### prepare data\n",
    "\n",
    "Pipeline:\n",
    "\n",
    "1. read data from file\n",
    "2. create data samples (called example here)\n",
    "3. convert examples to features (tokenization, padding, etc.)\n",
    "4. convert features to dataset (from list to tensor)\n",
    "5. create batch data\n",
    "\n",
    "Other functionalities:\n",
    "\n",
    "* save and load arguments (in practice, we use `argparse.ArgumentParser`)\n"
   ],
   "metadata": {
    "id": "4x55VvAHzdPs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#@title class PreProcessor\n",
    "# codes are revised from https://github.com/huggingface/transformers/blob/main/src/transformers/data/processors/glue.py\n",
    "class InputExample(object):\n",
    "    \"\"\"\n",
    "    A single training/test example for simple sequence classification.\n",
    "    Args:\n",
    "        guid: Unique id for the example.\n",
    "        text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "        text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "        label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(dataclasses.asdict(self), indent=2) + \"\\n\"\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"\n",
    "    A single set of features of data. Property names are the same names as the corresponding inputs to a model.\n",
    "    Args:\n",
    "        input_ids: Indices of input sequence tokens in the vocabulary.\n",
    "        attention_mask: Mask to avoid performing attention on padding token indices.\n",
    "            Mask values selected in `[0, 1]`: Usually `1` for tokens that are NOT MASKED, `0` for MASKED (padded)\n",
    "            tokens.\n",
    "        token_type_ids: (Optional) Segment token indices to indicate first and second\n",
    "            portions of the inputs. Only some models use them.\n",
    "        label: (Optional) Label corresponding to the input. Int for classification problems,\n",
    "            float for regression problems.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_ids, attention_mask=None, token_type_ids=None, label=None):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.label = label\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(dataclasses.asdict(self)) + \"\\n\"\n",
    "\n",
    "class PreProcessor():\n",
    "    \"\"\"Processor for the RTE data set (GLUE version).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_args = {'max_seq_length':64, 'verbose':False}\n",
    "\n",
    "    def save(self, path):\n",
    "        f = open(path, 'wb')\n",
    "        pickle.dump(self, f)\n",
    "        f.close()\n",
    "\n",
    "    def load(self, path):\n",
    "        f = open(path, 'rb')\n",
    "        proc = pickle.load(f)\n",
    "        f.close()\n",
    "        return proc\n",
    "    \n",
    "    def set_model_arg(self, key, value):\n",
    "        self.model_args[key] = value\n",
    "    \n",
    "    def get_model_arg(self, key):\n",
    "        return self.model_args.get(key, None)\n",
    "        return self.avg_seq_length\n",
    "    \n",
    "    def _read_tsv(self, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "            return list(csv.reader(f, delimiter=\"\\t\", quotechar=quotechar))\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "    \n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [\"entailment\", \"not_entailment\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, line[0])\n",
    "            label = line[-1]\n",
    "            text_a = line[1]\n",
    "            text_b = line[2]\n",
    "            \n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "    \n",
    "    def convert_examples_to_features(self, tokenizer, examples):\n",
    "        max_length = self.get_model_arg(\"max_seq_length\")\n",
    "        label_list = self.get_labels()\n",
    "        pad_token = tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0]\n",
    "        label_map = {label: i for i, label in enumerate(label_list)}\n",
    "        \n",
    "        features = []\n",
    "        for (ex_index, example) in enumerate(examples):\n",
    "            inputs = tokenizer.encode_plus(\n",
    "                example.text_a,\n",
    "                example.text_b,\n",
    "                truncation='longest_first',\n",
    "                add_special_tokens=True,\n",
    "                max_length=max_length,\n",
    "            )\n",
    "            input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "\n",
    "            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "            # tokens are attended to.\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "\n",
    "            # Zero-pad up to the sequence length.\n",
    "            padding_length = max_length - len(input_ids)\n",
    "\n",
    "            input_ids = input_ids + ([pad_token] * padding_length)\n",
    "            attention_mask = attention_mask + ([0] * padding_length)\n",
    "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "\n",
    "            assert len(input_ids) == max_length, \"Error with input length {} vs {}\".format(len(input_ids), max_length)\n",
    "            assert len(attention_mask) == max_length, \"Error with input length {} vs {}\".format(len(attention_mask), max_length)\n",
    "            assert len(token_type_ids) == max_length, \"Error with input length {} vs {}\".format(len(token_type_ids), max_length)\n",
    "\n",
    "            label = label_map[example.label]\n",
    "\n",
    "            if ex_index < 5 and self.get_model_arg('verbose'):\n",
    "                print(\"*** Example ***\")\n",
    "                print(\"guid: %s\" % (example.guid))\n",
    "                print(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "                print(\"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask]))\n",
    "                print(\"token_type_ids: %s\" % \" \".join([str(x) for x in token_type_ids]))\n",
    "                print(\"label: %s (id = %d)\" % (example.label, label))\n",
    "\n",
    "            features.append(\n",
    "                    InputFeatures(input_ids=input_ids,\n",
    "                                  attention_mask=attention_mask,\n",
    "                                  token_type_ids=token_type_ids,\n",
    "                                  label=label))\n",
    "        return features\n",
    "    \n",
    "    def convert_feature_to_dataset(self, features):\n",
    "        # Convert to Tensors and build dataset\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "        all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "        all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "\n",
    "        dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def get_dataloader(self, features, batch_size, is_test=False, drop_last=True):\n",
    "        dataset = self.convert_feature_to_dataset(features)\n",
    "\n",
    "        dataset_sampler = SequentialSampler(dataset) if is_test else RandomSampler(dataset)\n",
    "\n",
    "        dataloader = DataLoader(dataset, sampler=dataset_sampler, batch_size=batch_size, drop_last=drop_last)\n",
    "        return dataloader\n",
    "    \n",
    "    def get_data_iter(self, features, batch_size, is_test=False, drop_last=True):\n",
    "        dataloader = self.get_dataloader(features, batch_size, is_test=is_test, drop_last=(drop_last if not is_test else False))\n",
    "        return iter(dataloader)"
   ],
   "metadata": {
    "id": "Gv_keLWsdiLe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title prepare dataset and hyper-parameters for training\n",
    "proc = PreProcessor()\n",
    "\n",
    "# hyper-parameters for data\n",
    "proc.set_model_arg('batch_size', 8)\n",
    "proc.set_model_arg('max_seq_length', 256)\n",
    "# hyper-parameters for model\n",
    "proc.set_model_arg('learning_rate', 2e-5)\n",
    "proc.set_model_arg('n_epochs', 10)\n",
    "proc.set_model_arg('warmup_steps', 0.06)\n",
    "proc.set_model_arg('weight_decay', 0.1)\n",
    "proc.set_model_arg('adam_epsilon', 1e-8)\n",
    "proc.set_model_arg('clip', 1)\n",
    "\n",
    "# arguments for reproduction\n",
    "proc.set_model_arg('log_step', 150)\n",
    "proc.set_model_arg('verbose', True)    # if log details\n",
    "proc.set_model_arg('init_seed', 42)\n",
    "proc.set_model_arg('checkpoint_path', \"./RTE/plm_rte.bin\")\n",
    "proc.set_model_arg('dataset_path', \"./RTE/\")\n",
    "\n",
    "# save proc\n",
    "arg_path = \"./RTE/proc_rte.dat\"\n",
    "proc.save(arg_path)"
   ],
   "metadata": {
    "id": "qJ83U34LsecT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title utility functions\n",
    "\n",
    "def check_gpu():\n",
    "    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "    is_cuda = torch.cuda.is_available()\n",
    "\n",
    "    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "    if is_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"GPU is available\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"GPU not available, CPU used\")\n",
    "    return device\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    # random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model"
   ],
   "metadata": {
    "id": "Dm0JkRDPsV4z",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title preprare model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# get parameters from preprocessor\n",
    "init_seed = proc.get_model_arg('init_seed')\n",
    "proc.set_model_arg('verbose', False)\n",
    "\n",
    "device = check_gpu()\n",
    "set_seed(init_seed)\n",
    "\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "id": "hCOldxQZskT9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1684903472825,
     "user_tz": -480,
     "elapsed": 3446,
     "user": {
      "displayName": "Dr CAO Yixin _",
      "userId": "02213333189428679281"
     }
    },
    "outputId": "ecf724a1-d2fb-4bda-b724-8d70afe7e5f7"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU is available\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title finetuning function\n",
    "\n",
    "def train(model, tokenizer, proc, device):\n",
    "    # fetch hyper-parameters\n",
    "    batch_size = proc.get_model_arg(\"batch_size\")\n",
    "    n_epochs = proc.get_model_arg('n_epochs')\n",
    "    learning_rate = proc.get_model_arg(\"learning_rate\")\n",
    "    adam_epsilon = proc.get_model_arg('adam_epsilon')\n",
    "    weight_decay = proc.get_model_arg('weight_decay')\n",
    "    verbose = proc.get_model_arg(\"verbose\")\n",
    "    log_step = proc.get_model_arg(\"log_step\")\n",
    "    checkpoint_path = proc.get_model_arg(\"checkpoint_path\")\n",
    "    max_seq_length = proc.get_model_arg(\"max_seq_length\")\n",
    "    clip = proc.get_model_arg(\"clip\")\n",
    "    data_path = proc.get_model_arg(\"dataset_path\")\n",
    "    \n",
    "    # prepare training dataset\n",
    "    ######################################################\n",
    "    ####\n",
    "    #### 完善代码1：获得examples和features\n",
    "    #### \n",
    "    ######################################################\n",
    "    data_iter = proc.get_data_iter(features, batch_size)\n",
    "    # training steps in each epoch\n",
    "    examples_total_num = len(features)\n",
    "    max_steps = math.ceil(float(examples_total_num)/batch_size)\n",
    "    t_total = max_steps * n_epochs\n",
    "    \n",
    "    # Define Loss, Optimizer\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "    ######################################################\n",
    "    ####\n",
    "    #### 完善代码2：定义optimizer和scheduler\n",
    "    #### \n",
    "    ######################################################\n",
    "    \n",
    "\n",
    "    # train!\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.0\n",
    "        for step in range(max_steps):\n",
    "            model.train()\n",
    "            optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "            # prepare inputs\n",
    "            try:\n",
    "                batch = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = proc.get_data_iter(features, batch_size)\n",
    "                batch = next(data_iter)\n",
    "\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2],\n",
    "                      'labels':         batch[3]}\n",
    "\n",
    "            ######################################################\n",
    "            ####\n",
    "            #### 完善代码3：获得模型输出、获得loss、进行反向传播计算\n",
    "            #### \n",
    "            ######################################################\n",
    "            # clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate schedule\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            if step%log_step==0:\n",
    "                eval_metric = evaluate(model, tokenizer, proc, device)\n",
    "                print(\"step: {}/{}, Loss: {:.4f}, eval_metric: {}\".format(step, max_steps, loss.item(), eval_metric))\n",
    "\n",
    "        eval_metric = evaluate(model, tokenizer, proc, device)\n",
    "        print(\"epoch: {}/{}, Loss: {:.4f}, eval_metric: {}, saving model to {}\".format(epoch, n_epochs, total_loss/max_steps, eval_metric, checkpoint_path))\n",
    "        save_model(model, checkpoint_path)"
   ],
   "metadata": {
    "id": "nKbuCiMLshsg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title evaluation function\n",
    "def evaluate(model, tokenizer, proc, device, is_dev=True):\n",
    "    # fetch hyper-parameters\n",
    "    batch_size = proc.get_model_arg(\"batch_size\")\n",
    "    n_epochs = proc.get_model_arg('n_epochs')\n",
    "    learning_rate = proc.get_model_arg(\"learning_rate\")\n",
    "    adam_epsilon = proc.get_model_arg('adam_epsilon')\n",
    "    weight_decay = proc.get_model_arg('weight_decay')\n",
    "    verbose = proc.get_model_arg(\"verbose\")\n",
    "    log_step = proc.get_model_arg(\"log_step\")\n",
    "    checkpoint_path = proc.get_model_arg(\"checkpoint_path\")\n",
    "    max_seq_length = proc.get_model_arg(\"max_seq_length\")\n",
    "    clip = proc.get_model_arg(\"clip\")\n",
    "    data_path = proc.get_model_arg(\"dataset_path\")\n",
    "\n",
    "    metric = load_metric(\"glue\", \"rte\")\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # prepare dataset\n",
    "    examples = proc.get_dev_examples(data_path) if is_dev else proc.get_test_examples(data_path)\n",
    "    features = proc.convert_examples_to_features(tokenizer, examples)\n",
    "    eval_dataloader = proc.get_dataloader(features, batch_size)\n",
    "    \n",
    "    for batch in eval_dataloader:\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2],\n",
    "                      'labels':         batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "        predictions = outputs[1].argmax(dim=-1)\n",
    "        predictions, references = accelerator.gather((predictions, inputs[\"labels\"]))\n",
    "                    \n",
    "        metric.add_batch(\n",
    "                predictions=predictions,\n",
    "                references=references,\n",
    "            )\n",
    "        eval_metric = metric.compute()\n",
    "\n",
    "    return eval_metric"
   ],
   "metadata": {
    "id": "KFAxC9_a2gJp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title finetuning!\n",
    "train(model, tokenizer, proc, device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6UFXfSXf1g5P",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1684904866796,
     "user_tz": -480,
     "elapsed": 1387981,
     "user": {
      "displayName": "Dr CAO Yixin _",
      "userId": "02213333189428679281"
     }
    },
    "outputId": "67550cdf-ccea-4fc7-8f87-1d7197d74c0d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "step: 0/312, Loss: 1.1203, eval_metric: {'accuracy': 0.625}\n",
      "step: 150/312, Loss: 0.5492, eval_metric: {'accuracy': 0.625}\n",
      "step: 300/312, Loss: 0.5906, eval_metric: {'accuracy': 0.75}\n",
      "epoch: 0/10, Loss: 0.6824, eval_metric: {'accuracy': 0.5}, saving model to ./RTE/plm_rte.bin\n",
      "step: 0/312, Loss: 0.6215, eval_metric: {'accuracy': 0.75}\n",
      "step: 150/312, Loss: 0.6720, eval_metric: {'accuracy': 0.5}\n",
      "step: 300/312, Loss: 0.3125, eval_metric: {'accuracy': 0.5}\n",
      "epoch: 1/10, Loss: 0.5299, eval_metric: {'accuracy': 0.875}, saving model to ./RTE/plm_rte.bin\n",
      "step: 0/312, Loss: 0.2718, eval_metric: {'accuracy': 0.875}\n",
      "step: 150/312, Loss: 0.1526, eval_metric: {'accuracy': 0.625}\n",
      "step: 300/312, Loss: 0.3661, eval_metric: {'accuracy': 0.625}\n",
      "epoch: 2/10, Loss: 0.3500, eval_metric: {'accuracy': 0.625}, saving model to ./RTE/plm_rte.bin\n",
      "step: 0/312, Loss: 0.0516, eval_metric: {'accuracy': 0.5}\n",
      "step: 150/312, Loss: 0.0408, eval_metric: {'accuracy': 0.5}\n",
      "step: 300/312, Loss: 0.0086, eval_metric: {'accuracy': 0.5}\n",
      "epoch: 3/10, Loss: 0.2447, eval_metric: {'accuracy': 1.0}, saving model to ./RTE/plm_rte.bin\n",
      "step: 0/312, Loss: 0.5806, eval_metric: {'accuracy': 0.75}\n",
      "step: 150/312, Loss: 0.7166, eval_metric: {'accuracy': 0.625}\n",
      "step: 300/312, Loss: 0.0029, eval_metric: {'accuracy': 0.875}\n",
      "epoch: 4/10, Loss: 0.1554, eval_metric: {'accuracy': 0.75}, saving model to ./RTE/plm_rte.bin\n",
      "step: 0/312, Loss: 0.0087, eval_metric: {'accuracy': 0.5}\n",
      "step: 150/312, Loss: 0.0093, eval_metric: {'accuracy': 0.625}\n",
      "step: 300/312, Loss: 0.0010, eval_metric: {'accuracy': 0.75}\n",
      "epoch: 5/10, Loss: 0.0844, eval_metric: {'accuracy': 0.875}, saving model to ./RTE/plm_rte.bin\n",
      "step: 0/312, Loss: 0.0013, eval_metric: {'accuracy': 0.75}\n",
      "step: 150/312, Loss: 0.0011, eval_metric: {'accuracy': 0.25}\n",
      "step: 300/312, Loss: 0.0006, eval_metric: {'accuracy': 0.75}\n",
      "epoch: 6/10, Loss: 0.0545, eval_metric: {'accuracy': 0.875}, saving model to ./RTE/plm_rte.bin\n",
      "step: 0/312, Loss: 0.0004, eval_metric: {'accuracy': 0.5}\n",
      "step: 150/312, Loss: 0.0004, eval_metric: {'accuracy': 0.375}\n",
      "step: 300/312, Loss: 0.0005, eval_metric: {'accuracy': 0.875}\n",
      "epoch: 7/10, Loss: 0.0488, eval_metric: {'accuracy': 1.0}, saving model to ./RTE/plm_rte.bin\n",
      "step: 0/312, Loss: 0.0004, eval_metric: {'accuracy': 0.75}\n",
      "step: 150/312, Loss: 0.0003, eval_metric: {'accuracy': 0.75}\n",
      "step: 300/312, Loss: 0.0010, eval_metric: {'accuracy': 0.75}\n",
      "epoch: 8/10, Loss: 0.0654, eval_metric: {'accuracy': 0.625}, saving model to ./RTE/plm_rte.bin\n",
      "step: 0/312, Loss: 0.0004, eval_metric: {'accuracy': 0.5}\n",
      "step: 150/312, Loss: 0.0008, eval_metric: {'accuracy': 0.75}\n",
      "step: 300/312, Loss: 0.0013, eval_metric: {'accuracy': 0.875}\n",
      "epoch: 9/10, Loss: 0.0685, eval_metric: {'accuracy': 0.375}, saving model to ./RTE/plm_rte.bin\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Inference (load well-trained model)!\n",
    "\n",
    "# saved model for seq2seq with or without attention\n",
    "# use your own well-trained model\n",
    "checkpoint_path = \"./RTE/plm_rte.bin\"\n",
    "arg_path = \"./RTE/proc_rte.dat\"\n",
    "\n",
    "\n",
    "proc = PreProcessor()\n",
    "proc = proc.load(arg_path)\n",
    "\n",
    "# get parameters from preprocessor\n",
    "init_seed = proc.get_model_arg('init_seed')\n",
    "\n",
    "device = check_gpu()\n",
    "set_seed(init_seed)\n",
    "\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "# preprare model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "model = load_model(model, checkpoint_path)\n",
    "model = model.to(device)\n",
    "\n",
    "eval_metric = evaluate(model, tokenizer, proc, device, is_dev=False)\n",
    "print(\"eval_metric: {}\".format(eval_metric))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ZU5VszJKWOk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1684906609901,
     "user_tz": -480,
     "elapsed": 5444,
     "user": {
      "displayName": "Dr CAO Yixin _",
      "userId": "02213333189428679281"
     }
    },
    "outputId": "d2a9e001-30dd-4499-9e41-19e2c9c2acae"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU is available\n",
      "*** Example ***\n",
      "guid: test-176\n",
      "input_ids: 101 2149 2343 13857 8112 2038 2042 11080 2075 2041 1996 2925 3257 1997 2010 3447 1010 1999 2824 2000 2928 2010 2034 2531 2420 1999 2436 1012 2720 8112 8280 1037 2237 2534 3116 1999 2358 3434 2077 3173 1037 18474 2739 3034 1010 3743 2444 2006 2087 2149 2694 6125 1012 6964 1010 15957 2031 2109 1996 19199 2000 14358 11274 1005 2220 14152 1998 15428 1012 6745 14592 6592 2720 8112 4247 2000 5959 1037 2152 2504 1997 6217 1012 102 13857 8112 2038 2042 2343 1997 1996 1057 1012 1055 1012 2005 2531 2420 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: entailment (id = 0)\n",
      "*** Example ***\n",
      "guid: test-177\n",
      "input_ids: 101 10503 2038 11883 2005 3087 2040 2038 2439 2037 3268 1999 5712 1012 102 10503 2003 3374 2005 3087 2040 2038 2439 2037 3268 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: entailment (id = 0)\n",
      "*** Example ***\n",
      "guid: test-178\n",
      "input_ids: 101 2096 5747 2001 1999 1996 2250 1010 1996 2317 2160 2001 13377 1010 2007 2116 5126 2770 2185 2013 1996 10345 2006 4449 2013 3595 2326 6074 1012 102 1996 1057 1012 1055 1012 2510 13377 1057 1012 1055 1012 4480 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: not_entailment (id = 1)\n",
      "*** Example ***\n",
      "guid: test-179\n",
      "input_ids: 101 2055 2431 2020 2247 1037 2322 1011 3542 7683 1997 4203 9018 3016 2013 2327 18222 8399 8459 2000 1996 24326 2015 16184 2015 6000 1012 102 1996 15458 1997 4203 9018 3016 2003 2753 2661 2146 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: not_entailment (id = 1)\n",
      "*** Example ***\n",
      "guid: test-180\n",
      "input_ids: 101 1996 19337 5243 9077 5596 9944 1011 6373 4323 2380 2648 3000 2003 2725 2061 9996 2009 2453 2031 2000 2485 4983 2009 4152 2393 2574 2013 2049 18496 2545 1010 1996 3472 1997 10598 6373 2522 1012 2056 1999 2019 4357 2405 5958 1012 102 9944 1011 6373 2003 1037 4323 2380 2648 3000 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: entailment (id = 0)\n",
      "eval_metric: {'accuracy': 0.625}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create your own model\n",
    "\n",
    "Now, can you build your own model based on BERT by adding additional output layer!\n",
    "\n",
    "We use BertModel class, which has two outputs:\n",
    "\n",
    "\n",
    "\n",
    "1.   output[0]: sequence_output (batch_size, sequence_length, 768) contains all tokens' hidden states in the last layer\n",
    "2.   output[1]: pooled_output (batch_size, 768) is the hidden states of the [CLS] token in the layer, which is regarded as a summary of the content according to the entire input sequence.\n",
    "\n"
   ],
   "metadata": {
    "id": "kKL1AZDD6nTP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self, num_labels, dropout=0.1):\n",
    "          super(CustomBERTModel, self).__init__()\n",
    "          self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "          ### New layers:\n",
    "          self.num_labels = num_labels\n",
    "          self.dropout = nn.Dropout(dropout)\n",
    "          self.classifier = nn.Linear(768, self.num_labels) ## 2 is the number of classes\n",
    "\n",
    "          self.loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, \n",
    "                input_ids: Optional[torch.Tensor] = None,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                token_type_ids: Optional[torch.Tensor] = None,\n",
    "                labels: Optional[torch.Tensor] = None,\n",
    "            ):\n",
    "          outputs = self.bert(\n",
    "          ######################################################\n",
    "          ####\n",
    "          #### 完善代码4：填写参数\n",
    "          #### \n",
    "          ######################################################     \n",
    "          )\n",
    "\n",
    "          # sequence_output has the following shape: (batch_size, sequence_length, 768)\n",
    "          # pooled_output has the following shape: (batch_size, 768)\n",
    "          sequence_output = outputs[0]\n",
    "          pooled_output = outputs[1]\n",
    "          \n",
    "          pooled_output = self.dropout(pooled_output)\n",
    "          logits = self.classifier(pooled_output)\n",
    "\n",
    "          loss = self.loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "          return loss, logits"
   ],
   "metadata": {
    "id": "fB5ynGHP3Jai"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title prepare dataset and hyper-parameters for training\n",
    "custom_proc = PreProcessor()\n",
    "\n",
    "# hyper-parameters for data\n",
    "custom_proc.set_model_arg('batch_size', 8)\n",
    "custom_proc.set_model_arg('max_seq_length', 256)\n",
    "# hyper-parameters for model\n",
    "custom_proc.set_model_arg('learning_rate', 2e-5)\n",
    "custom_proc.set_model_arg('n_epochs', 10)\n",
    "custom_proc.set_model_arg('warmup_steps', 0.06)\n",
    "custom_proc.set_model_arg('weight_decay', 0.1)\n",
    "custom_proc.set_model_arg('adam_epsilon', 1e-8)\n",
    "custom_proc.set_model_arg('clip', 1)\n",
    "\n",
    "# arguments for reproduction\n",
    "custom_proc.set_model_arg('log_step', 150)\n",
    "custom_proc.set_model_arg('verbose', False)    # if log details\n",
    "custom_proc.set_model_arg('init_seed', 42)\n",
    "custom_proc.set_model_arg('checkpoint_path', \"./RTE/custom_plm_rte.bin\")\n",
    "custom_proc.set_model_arg('dataset_path', \"./RTE/\")\n",
    "\n",
    "# save proc\n",
    "arg_path = \"./RTE/custom_proc_rte.dat\"\n",
    "custom_proc.save(arg_path)"
   ],
   "metadata": {
    "id": "5dyEBP0vRfA6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title preprare model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "custom_model = CustomBERTModel(num_labels = 2)\n",
    "\n",
    "# get parameters from preprocessor\n",
    "init_seed = proc.get_model_arg('init_seed')\n",
    "\n",
    "device = check_gpu()\n",
    "set_seed(init_seed)\n",
    "\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "custom_model = custom_model.to(device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4sa-lBhPRy0S",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1684908981862,
     "user_tz": -480,
     "elapsed": 3875,
     "user": {
      "displayName": "Dr CAO Yixin _",
      "userId": "02213333189428679281"
     }
    },
    "outputId": "beb6c400-42c1-4072-a103-c9a85f8f60a9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU is available\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title finetuning!\n",
    "train(custom_model, tokenizer, custom_proc, device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DoO9-REUSEXq",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1684910307930,
     "user_tz": -480,
     "elapsed": 1322563,
     "user": {
      "displayName": "Dr CAO Yixin _",
      "userId": "02213333189428679281"
     }
    },
    "outputId": "b5385da6-0567-46b5-c4ea-359324e61ebf"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "step: 0/312, Loss: 0.7407, eval_metric: {'accuracy': 0.625}\n",
      "step: 150/312, Loss: 0.6392, eval_metric: {'accuracy': 0.625}\n",
      "step: 300/312, Loss: 0.6790, eval_metric: {'accuracy': 0.75}\n",
      "epoch: 0/10, Loss: 0.6931, eval_metric: {'accuracy': 0.25}, saving model to ./RTE/custom_plm_rte.bin\n",
      "step: 0/312, Loss: 0.7336, eval_metric: {'accuracy': 0.25}\n",
      "step: 150/312, Loss: 0.6842, eval_metric: {'accuracy': 0.875}\n",
      "step: 300/312, Loss: 0.4764, eval_metric: {'accuracy': 0.375}\n",
      "epoch: 1/10, Loss: 0.5973, eval_metric: {'accuracy': 0.5}, saving model to ./RTE/custom_plm_rte.bin\n",
      "step: 0/312, Loss: 0.3204, eval_metric: {'accuracy': 0.625}\n",
      "step: 150/312, Loss: 0.2186, eval_metric: {'accuracy': 0.75}\n",
      "step: 300/312, Loss: 0.3987, eval_metric: {'accuracy': 0.625}\n",
      "epoch: 2/10, Loss: 0.4050, eval_metric: {'accuracy': 0.625}, saving model to ./RTE/custom_plm_rte.bin\n",
      "step: 0/312, Loss: 0.1111, eval_metric: {'accuracy': 0.75}\n",
      "step: 150/312, Loss: 0.0424, eval_metric: {'accuracy': 0.875}\n",
      "step: 300/312, Loss: 0.0185, eval_metric: {'accuracy': 1.0}\n",
      "epoch: 3/10, Loss: 0.2756, eval_metric: {'accuracy': 0.875}, saving model to ./RTE/custom_plm_rte.bin\n",
      "step: 0/312, Loss: 0.5432, eval_metric: {'accuracy': 0.75}\n",
      "step: 150/312, Loss: 0.6776, eval_metric: {'accuracy': 0.625}\n",
      "step: 300/312, Loss: 0.7232, eval_metric: {'accuracy': 0.75}\n",
      "epoch: 4/10, Loss: 0.1876, eval_metric: {'accuracy': 0.625}, saving model to ./RTE/custom_plm_rte.bin\n",
      "step: 0/312, Loss: 0.0044, eval_metric: {'accuracy': 0.5}\n",
      "step: 150/312, Loss: 0.0029, eval_metric: {'accuracy': 0.5}\n",
      "step: 300/312, Loss: 0.0008, eval_metric: {'accuracy': 0.625}\n",
      "epoch: 5/10, Loss: 0.0989, eval_metric: {'accuracy': 0.875}, saving model to ./RTE/custom_plm_rte.bin\n",
      "step: 0/312, Loss: 0.0011, eval_metric: {'accuracy': 0.625}\n",
      "step: 150/312, Loss: 0.0005, eval_metric: {'accuracy': 0.625}\n",
      "step: 300/312, Loss: 0.0005, eval_metric: {'accuracy': 0.875}\n",
      "epoch: 6/10, Loss: 0.0530, eval_metric: {'accuracy': 0.5}, saving model to ./RTE/custom_plm_rte.bin\n",
      "step: 0/312, Loss: 0.0004, eval_metric: {'accuracy': 0.875}\n",
      "step: 150/312, Loss: 0.0004, eval_metric: {'accuracy': 0.5}\n",
      "step: 300/312, Loss: 0.0003, eval_metric: {'accuracy': 0.75}\n",
      "epoch: 7/10, Loss: 0.0467, eval_metric: {'accuracy': 0.75}, saving model to ./RTE/custom_plm_rte.bin\n",
      "step: 0/312, Loss: 0.0006, eval_metric: {'accuracy': 0.875}\n",
      "step: 150/312, Loss: 0.0003, eval_metric: {'accuracy': 0.875}\n",
      "step: 300/312, Loss: 0.0005, eval_metric: {'accuracy': 0.875}\n",
      "epoch: 8/10, Loss: 0.0519, eval_metric: {'accuracy': 0.625}, saving model to ./RTE/custom_plm_rte.bin\n",
      "step: 0/312, Loss: 0.0002, eval_metric: {'accuracy': 1.0}\n",
      "step: 150/312, Loss: 0.0019, eval_metric: {'accuracy': 0.625}\n",
      "step: 300/312, Loss: 0.0003, eval_metric: {'accuracy': 0.5}\n",
      "epoch: 9/10, Loss: 0.0352, eval_metric: {'accuracy': 0.875}, saving model to ./RTE/custom_plm_rte.bin\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Inference (load well-trained model)! You may need to tune your hyperparameters and architectures for better performance!\n",
    "\n",
    "# saved model for seq2seq with or without attention\n",
    "# use your own well-trained model\n",
    "checkpoint_path = \"./RTE/custom_plm_rte.bin\"\n",
    "arg_path = \"./RTE/custom_proc_rte.dat\"\n",
    "\n",
    "\n",
    "custom_proc = PreProcessor()\n",
    "custom_proc = custom_proc.load(arg_path)\n",
    "\n",
    "# get parameters from preprocessor\n",
    "init_seed = custom_proc.get_model_arg('init_seed')\n",
    "\n",
    "device = check_gpu()\n",
    "set_seed(init_seed)\n",
    "\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "# preprare model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "custom_model = CustomBERTModel(num_labels = 2)\n",
    "custom_model = load_model(custom_model, checkpoint_path)\n",
    "custom_model = custom_model.to(device)\n",
    "\n",
    "eval_metric = evaluate(custom_model, tokenizer, custom_proc, device, is_dev=False)\n",
    "print(\"eval_metric: {}\".format(eval_metric))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DEVOLb2kpJfd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1684913139822,
     "user_tz": -480,
     "elapsed": 8567,
     "user": {
      "displayName": "Dr CAO Yixin _",
      "userId": "02213333189428679281"
     }
    },
    "outputId": "e5074002-0f4d-4dd3-ba15-753dc0a58e62"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU is available\n",
      "eval_metric: {'accuracy': 0.375}\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "e4d621fb5c4a4f1f95b8dd9d1880bfc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_91faa8e3badb43409bb8317ebf9e660e",
       "IPY_MODEL_f94088ac96ca4b3284c0afa4e1b1d5d1",
       "IPY_MODEL_a3e5154ab9e4436a877bfcfeccabe1b9"
      ],
      "layout": "IPY_MODEL_610005c89d9447e182ca6b17f357b536"
     }
    },
    "91faa8e3badb43409bb8317ebf9e660e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f95e593057904a7a90dec4a74481936f",
      "placeholder": "​",
      "style": "IPY_MODEL_afbe7115f0094c918eb1c62dbc46cec1",
      "value": "Downloading (…)okenizer_config.json: 100%"
     }
    },
    "f94088ac96ca4b3284c0afa4e1b1d5d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bdbbbe8b7e9148e397b6ab8ab4583c7e",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6756e4c228404afcac500e1ec73bbd28",
      "value": 28
     }
    },
    "a3e5154ab9e4436a877bfcfeccabe1b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1bf7564c134d4102b6e36396d3e6010a",
      "placeholder": "​",
      "style": "IPY_MODEL_aba8686e374344fd92c54be6ffcf6b06",
      "value": " 28.0/28.0 [00:00&lt;00:00, 1.36kB/s]"
     }
    },
    "610005c89d9447e182ca6b17f357b536": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f95e593057904a7a90dec4a74481936f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "afbe7115f0094c918eb1c62dbc46cec1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bdbbbe8b7e9148e397b6ab8ab4583c7e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6756e4c228404afcac500e1ec73bbd28": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1bf7564c134d4102b6e36396d3e6010a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aba8686e374344fd92c54be6ffcf6b06": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "768af08be5f14513adb3630e022da6f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b5b4c15d22064d3389e60667b97f2ebe",
       "IPY_MODEL_829671f4f2064c7a9124ea3fa8b2bdb5",
       "IPY_MODEL_ce36752e5aa54b9eaff08dfca8a86daa"
      ],
      "layout": "IPY_MODEL_2f97926f132946c0b57c3bda1c779d94"
     }
    },
    "b5b4c15d22064d3389e60667b97f2ebe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ad341e0d068424383537d9064cff21f",
      "placeholder": "​",
      "style": "IPY_MODEL_347e38295a4f46deaf2dad05c2c90ac8",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "829671f4f2064c7a9124ea3fa8b2bdb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_949d9179d15a45a584d0753181f3c939",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b75ffda82d7a44eca6116331fc044c00",
      "value": 570
     }
    },
    "ce36752e5aa54b9eaff08dfca8a86daa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_507fd8aa9d1b429886ab0e3bf42ffa02",
      "placeholder": "​",
      "style": "IPY_MODEL_d5430bb1f217419b820e26cd029b4291",
      "value": " 570/570 [00:00&lt;00:00, 7.55kB/s]"
     }
    },
    "2f97926f132946c0b57c3bda1c779d94": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ad341e0d068424383537d9064cff21f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "347e38295a4f46deaf2dad05c2c90ac8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "949d9179d15a45a584d0753181f3c939": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b75ffda82d7a44eca6116331fc044c00": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "507fd8aa9d1b429886ab0e3bf42ffa02": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5430bb1f217419b820e26cd029b4291": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "51ccb77a3715446cb3a56a33e7f714ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5f6e21b3ad4d44e0ab3e141c51020979",
       "IPY_MODEL_8b88cbf5d89e467c8bb8384b3dae1bbd",
       "IPY_MODEL_23a22917e8d14145961544f31805a3e4"
      ],
      "layout": "IPY_MODEL_a7c8149c62164561b059fdd75aec7cfe"
     }
    },
    "5f6e21b3ad4d44e0ab3e141c51020979": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed33ebf045ef4cba9d701d1b095fdc73",
      "placeholder": "​",
      "style": "IPY_MODEL_2603f9a0d3d54d9d821f4de029810660",
      "value": "Downloading (…)solve/main/vocab.txt: 100%"
     }
    },
    "8b88cbf5d89e467c8bb8384b3dae1bbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba4d2a9337a54accbdc742714532a48c",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8f865859b89d40ca8c36431d71cc56f2",
      "value": 231508
     }
    },
    "23a22917e8d14145961544f31805a3e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ad61a856afe4bb58ac3491d405b0628",
      "placeholder": "​",
      "style": "IPY_MODEL_695bc58e0d74493b82c9ffb4e7335bc8",
      "value": " 232k/232k [00:00&lt;00:00, 1.10MB/s]"
     }
    },
    "a7c8149c62164561b059fdd75aec7cfe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed33ebf045ef4cba9d701d1b095fdc73": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2603f9a0d3d54d9d821f4de029810660": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba4d2a9337a54accbdc742714532a48c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f865859b89d40ca8c36431d71cc56f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8ad61a856afe4bb58ac3491d405b0628": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "695bc58e0d74493b82c9ffb4e7335bc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c80925436eb94b5895cc436bb54ceb6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b02ad3c5a27f4936a92d33f2cd2063d4",
       "IPY_MODEL_dd21df22626c4aa9aff11f0eb8797648",
       "IPY_MODEL_4546c2b074e04f88b0332fbf3fd6feba"
      ],
      "layout": "IPY_MODEL_b570c8c2ad7544b99cc481ebfccf837a"
     }
    },
    "b02ad3c5a27f4936a92d33f2cd2063d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75188feeee7045b59e40d0c57940e400",
      "placeholder": "​",
      "style": "IPY_MODEL_f5c93d3d3322400094e4e4aa6f4ffcdd",
      "value": "Downloading (…)/main/tokenizer.json: 100%"
     }
    },
    "dd21df22626c4aa9aff11f0eb8797648": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_576d73b34397431881558516e7dc2168",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c75d1f618f774367bfab5b04b6634d35",
      "value": 466062
     }
    },
    "4546c2b074e04f88b0332fbf3fd6feba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae67b233395d409b9dfd6b94e4c8a76a",
      "placeholder": "​",
      "style": "IPY_MODEL_ec02a2a4f0a84336b9bd67cf6e93ab5c",
      "value": " 466k/466k [00:00&lt;00:00, 730kB/s]"
     }
    },
    "b570c8c2ad7544b99cc481ebfccf837a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75188feeee7045b59e40d0c57940e400": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5c93d3d3322400094e4e4aa6f4ffcdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "576d73b34397431881558516e7dc2168": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c75d1f618f774367bfab5b04b6634d35": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ae67b233395d409b9dfd6b94e4c8a76a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec02a2a4f0a84336b9bd67cf6e93ab5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
